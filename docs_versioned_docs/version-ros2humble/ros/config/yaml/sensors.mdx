---
title: Sensors
sidebar_label: Sensors
sidebar_position: 7
toc_min_heading_level: 2
toc_max_heading_level: 4
---
import CHRoboticsUM6 from "/docs_versioned_docs/version-ros2humble/components/yaml/sensors/chrobotics_um6.mdx";
import FlirBlackfly from "/docs_versioned_docs/version-ros2humble/components/yaml/sensors/flir_blackfly.mdx";
import Garmin18x from "/docs_versioned_docs/version-ros2humble/components/yaml/sensors/garmin_18x.mdx";
import HokuyoUST from "/docs_versioned_docs/version-ros2humble/components/yaml/sensors/hokuyo_ust.mdx";
import IntelRealsense from "/docs_versioned_docs/version-ros2humble/components/yaml/sensors/intel_realsense.mdx";
import MicrostrainIMU from "/docs_versioned_docs/version-ros2humble/components/yaml/sensors/microstrain_imu.mdx";
import NovatelSmart6 from "/docs_versioned_docs/version-ros2humble/components/yaml/sensors/novatel_smart6.mdx";
import NovatelSmart7 from "/docs_versioned_docs/version-ros2humble/components/yaml/sensors/novatel_smart7.mdx";
import RedShiftUM7 from "/docs_versioned_docs/version-ros2humble/components/yaml/sensors/redshift_um7.mdx";
import SickLMS1xx from "/docs_versioned_docs/version-ros2humble/components/yaml/sensors/sick_lms1xx.mdx";
import SwiftNavDuro from "/docs_versioned_docs/version-ros2humble/components/yaml/sensors/swiftnav_duro.mdx";
import VelodyneLidar from "/docs_versioned_docs/version-ros2humble/components/yaml/sensors/velodyne_lidar.mdx";

At Clearpath, we have been migrating our large inventory of tested sensor drivers from ROS 1 to ROS 2.

Sensors are split up into sections:

- **Cameras:** publish **_sensor_msgs/Image_** messages
- **GPS:** publish **_sensor_msgs/NavSatFix_** messages
- **IMU:** publish **_sensor_msgs/Imu_** messages
- **LiDAR 2D:** publish **_sensor_msgs/LaserScan_** messages
- **LiDAR 3D:** publish **_sensor_msgs/PointCloud2_** messages

In ROS 2, sensors use a `ros_parameters` YAML that contains all launch parameters for the driver node. To facilitate complete customization of these node parameters, the `ros_parameters` section, under every sensor entry, serves as a way to pass those key-value pairs to the corresponding node.

By default, we pass tested parameters that are used on Clearpath robots.

## Cameras

### Compressed and Theora Topics
Using the `image_transport` package and the [image_transport_plugins](https://github.com/ros-perception/image_transport_plugins), it is possible to send compressed and stream images using ROS 2 messages.

For example, the color image topic will have multiple versions:
```bash
/a200_0000/sensors/camera_0/color/image
/a200_0000/sensors/camera_0/color/compressed
/a200_0000/sensors/camera_0/color/theora
```

:::note

The same publisher handles all three of these message types; therefore, limit number of subscribers to minimize the load on the camera driver node.

:::

Compressed and theora types are not standard ROS 2 types, and therefore, it is required to decompress the images into standard `sensor_msgs/Image` messages before attempting to view them using standard tools such as RViz.

The `clearpath_sensors` package has launch files to convert from and to these compressed transport types. Therefore, make sure to have the package installed:
```bash
sudo apt install ros-humble-clearpath-sensors
```

#### Compressed
Compressed image types are not serialized. These are images compressed using the JPEG format at 80% quality.

Use the launch file `image_raw_to_compressed.launch.py` to compress `sensor_msgs/Image` messages into compressed messages.

Assume we have the following image topic we would like to compress:
```bash
/a200_0000/sensors/camera_0/raw/image
```

Then, we can compress this image by launching the following:
```bash
ros2 launch clearpath_sensors image_raw_to_compressed.launch.py in_raw:=/a200_0000/sensors/camera_0/raw/image out_compressed:=/a200_0000/sensors/camera_0/raw/compressed
```

Now, we will have the following topics:
```bash
/a200_0000/sensors/camera_0/raw/image
/a200_0000/sensors/camera_0/raw/compressed
```

To reverse this process, use the `image_compressed_to_raw.launch.py` to decompress back to `sensor_msgs/Image`:
```bash
ros2 launch clearpath_sensors image_compressed_to_raw.launch.py in_compressed:=/a200_0000/sensors/camera_0/raw/compressed out_raw:=/a200_0000/sensors/camera_0/decompressed/image
```

Now, we will have the following topics:
```bash
/a200_0000/sensors/camera_0/raw/image
/a200_0000/sensors/camera_0/raw/compressed
/a200_0000/sensors/camera_0/decompressed/image
```

#### Theora
Theora topics encode images into a video stream. By default, these streams have an 800 kb/s rate.

Theora messages are not serialized, however, the first message after launching the encoder transmits the header packet. This header packet includes all of the encoding information required to decode the subsequent video packets. Therefore, if that first message is dropped or missed, then decoding the video packets back to images is not possible.

Therefore, it is important to have the decoder node running before launching the encoder node, such that the former will not miss the header packet.

:::warning
Missing the header packet and being unable to decode the video packets is a known, long-standing issue with the `theora` image transport plugin. See this [issue](https://github.com/ros-perception/image_transport_plugins/issues/4) for more information.
:::

Assuming we have the following topic that we would like to stream:
```bash
/a200_0000/sensors/camera_0/raw/image
```

First, launch the decoder:
```bash
ros2 launch clearpath_sensors image_theora_to_raw.launch.py in_theora:=/a200_0000/sensors/camera_0/raw/theora out_raw:=/a200_0000/sensors/camera_0/decoded/image
```

Then, launch the encoder:
```bash
ros2 launch clearpath_sensors image_raw_to_theora.launch.py in_raw:=/a200_0000/sensors/camera_0/raw/image out_raw:=/a200_0000/sensors/camera_0/raw/theora
```

As long as the first header package is communicated without drops, the decoder will be able to decode and publish the raw image data:
```bash
/a200_0000/sensors/camera_0/raw/image
/a200_0000/sensors/camera_0/raw/theora
/a200_0000/sensors/camera_0/decoded/image
```

### Republishers
All cameras publish to the `color/image` topic of type `sensor_msgs/Image`. However, we understand that sometimes this image may require downsampling, rectification, or cropping. To facilitate post processing images, we have included a method to easily add image processing nodes, leveraging composable nodes to maximize efficiency.

:::note

For more information on the image processing nodes see the [ROS wiki](https://wiki.ros.org/image_proc).

:::

Adding a republisher is simple.
- Add a `republishers` tag under the `camera` entry.
- Specify the `type` of republisher: `rectify` or `resize`.
- Specify the `input` topic namespace. By default, it is `color` because the standard image topic will be `color/image`.
- Specify the `output` topic namespace. By default, it is the type of the republisher: `{type}/image`.

Configuring a republisher can be done by setting its node parameters using the `ros_parameters` section.
- Under `ros_parameters` add an entry for the node `image_TYPE_INPUT`. Note, the node name depends on the `type` of republisher and the `input` name.
- Add any parameter key, value pair to configure the node. Otherwise, it can be left empty to use defaults.

### Rectify
The `rectify` node takes the `camera_info` topic as a source of calibration parameters and applies an interpolation to rectify the raw images.

To add a the rectify republisher add the following entries to the `camera` entry and its corresponding `ros_parameters` section.
```yaml
cameras:
  - model: CAMERA_MODEL
    republishers:
      - type: rectify
        input: color
        output: rectify
    ros_parameters:
      CAMERA_NODE:
        CAMERA_PARAMETER_KEY: CAMERA_PARAMETER_VALUE
      image_rectify_color:
        interpolation: 0
```

The `interpolation` parameter must be one of the following:
- **0**: Nearest-neighbour.
- **1**: Linear.
- **2**: Cubic.
- **3**: Area. Resampling using pixel area relation.
- **4**: Lanczos4. Lanczos interpolation over 8x8 neighbourhood.

:::note

All UPPERCASE entries must be replaced based on the specific camera being used.

:::

### Resize
The `resize` node uses the input image and resizes it.

To add a resize republisher add teh following entreis to the `camera` entry and its corresponding `ros_parameters` section.
```yaml
cameras:
  - model: CAMERA_MODEL
    republishers:
      - type: resize
        input: color
        output: resize
    ros_parameters:
      CAMERA_NODE:
        CAMERA_PARAMETER_KEY: CAMERA_PARAMETER_VALUE
      image_resize_color:
        interpolation: 1
        use_scale: True
        scale_height: 1.0
        scale_width: 1.0
        height: -1
        width: -1
```
If `use_scale` is set to `true`, then the `scale_height` and `scale_width` parameters will be used to resize the image. To reduce the size of the image by half, set `scale_height: 0.5` and `scale_width: 0.5`.

If `use_scale` is set to `false`, then the `height` and `width` parameters will be used to resize the image. In this case, the exact pixel values of the desired image size can be passed.

:::note

The `scale_height` and `scale_width` parameters must be floating point values.

The `height` and `width` parameters must be integer values.

:::

The `interpolation` parameters must be one of the following:
- **0**: Nearest-neighbour.
- **1**: Linear.
- **2**: Cubic.
- **3**: Area. Resampling using pixel area relation.
- **4**: Lanczos4. Lanczos interpolation over 8x8 neighbourhood.

### Intel Realsense
<IntelRealsense/>

### Flir BlackflyS
<FlirBlackfly/>

## GPS
### SwiftNav Duro
<SwiftNavDuro/>

### Garmin 18X
<Garmin18x/>

## Novatel Smart6
<NovatelSmart6/>

## Novatel Smart7
<NovatelSmart7/>

## IMU
### Microstrain IMU
<MicrostrainIMU/>

### CHRobotics UM6
<CHRoboticsUM6/>

### Redshift UM7
<RedShiftUM7/>

## LiDAR 2D
### Hokuyo UST
<HokuyoUST/>

### SICK LMS1xx
<SickLMS1xx/>

## LiDAR 3D
### Velodyne Lidar
<VelodyneLidar/>

## Sample

<details><summary>Sample A200 Sensors Section</summary>
<p>

<center>
  <figure>
    <img
      src={require("../img/husky_mounts_1.png").default}
      width="500"
    />
    <figcaption>Husky A200 with upside down Fath Pivot Mount</figcaption>
  </figure>
</center>

In this sample, we first add the `velodyne_lidar` to the `sensor_arch_mount` by simply changing the parent link.

```yaml
lidar3d:
  - model: velodyne_lidar
    urdf_enabled: true
    launch_enabled: true
    parent: sensor_arch_mount
    xyz: [0.0, 0.0, 0.0]
    rpy: [0.0, 0.0, 0.0]
    ros_parameters:
      velodyne_driver_node:
        frame_id: lidar3d_0_laser
        device_ip: 192.168.131.25
        port: 2368
        model: VLP16
      velodyne_transform_node:
        model: VLP16
        fixed_frame: lidar3d_0_laser
        target_frame: lidar3d_0_laser
```

<center>
  <figure>
    <img
      src={require("../img/husky_sensors_0.png").default}
      width="500"
    />
    <figcaption>Husky A200 with LiDAR 3D on Sensor Arch</figcaption>
  </figure>
</center>

Next, we will add a `hokuyo_ust` to the **bracket** we added earlier. Since that is the first **bracket**, then it's mounting location will be: `bracket_0_mount`; setting the parent link of the sensor, we get:

```yaml
lidar2d:
  - model: hokuyo_ust
    urdf_enabled: true
    launch_enabled: true
    parent: bracket_0_mount
    xyz: [0.0, 0.0, 0.0]
    rpy: [0.0, 0.0, 0.0]
    ros_parameters:
      urg_node:
        laser_frame_id: lidar2d_0_laser
        ip_address: 192.168.131.20
        ip_port: 10940
        angle_min: -2.356
        angle_max: 2.356
```

<center>
  <figure>
    <img
      src={require("../img/husky_sensors_1.png").default}
      width="500"
    />
    <figcaption>Husky A200 with LiDAR 2D on D1 Bracket</figcaption>
  </figure>
</center>

For the final step, we will add an `intel_realsense` to the **fath_pivot** mount that we added. Because it is the first **fath_pivot**, it's mounting location will be: `fath_pivot_0_mount`; setting the parent link of the sensor:

```yaml
camera:
  - model: intel_realsense
    urdf_enabled: true
    launch_enabled: true
    parent: fath_pivot_0_mount
    xyz: [0.0, 0.0, 0.0]
    rpy: [0.0, 0.0, 0.0]
    ros_parameters:
      camera:
        camera_name: camera_0
        device_type: d435
        serial_no: '0'
        enable_color: true
        rgb_camera.profile: 640,480,30
        enable_depth: true
        depth_module.profile: 640,480,30
        pointcloud.enable: true
```

<center>
  <figure>
    <img
      src={require("../img/husky_sensors_2.png").default}
      width="500"
    />
    <figcaption>Husky A200 with Intel RealSense</figcaption>
  </figure>
</center>

Leaving the other sections empty, leaves us with the full sensors section:

```yaml
sensors:
  camera:
    - model: intel_realsense
      urdf_enabled: true
      launch_enabled: true
      parent: fath_pivot_0_mount
      xyz: [0.0, 0.0, 0.0]
      rpy: [0.0, 0.0, 0.0]
      ros_parameters:
        camera:
          camera_name: camera_0
          device_type: d435
          serial_no: '0'
          enable_color: true
          rgb_camera.profile: 640,480,30
          enable_depth: true
          depth_module.profile: 640,480,30
          pointcloud.enable: true
  gps: []
  imu: []
  lidar2d:
    - model: hokuyo_ust
      urdf_enabled: true
      launch_enabled: true
      parent: bracket_0_mount
      xyz: [0.0, 0.0, 0.0]
      rpy: [0.0, 0.0, 0.0]
      ros_parameters:
        urg_node:
          laser_frame_id: lidar2d_0_laser
          ip_address: 192.168.131.20
          ip_port: 10940
          angle_min: -2.356
          angle_max: 2.356
  lidar3d:
    - model: velodyne_lidar
      urdf_enabled: true
      launch_enabled: true
      parent: sensor_arch_mount
      xyz: [0.0, 0.0, 0.0]
      rpy: [0.0, 0.0, 0.0]
      ros_parameters:
        velodyne_driver_node:
          frame_id: lidar3d_0_laser
          device_ip: 192.168.131.25
          port: 2368
          model: VLP16
        velodyne_transform_node:
          model: VLP16
          fixed_frame: lidar3d_0_laser
          target_frame: lidar3d_0_laser

```

</p>
</details>
